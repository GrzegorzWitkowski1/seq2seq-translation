{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Polish to English neural machine translation\n",
        "The seq2seq model architecture was written by google developers and is licensed under the Apache 2.0 License. I refitted the code to handle polish-to-english translation for long samples of text looping the original seq2seq model. It takes polish sentences with polish charecters as input and returns properly formatted english sentences. The training data was sourced from the Anki database containing sentence pairs from the Tatoeba Project. I have also implemented a mechanism to correct the model. Method `Translation.teach_model()` allows users to assess the models translation and label it as correct or incorrect and then provide the correct translation. The newly created context -> target pair then gets added to the models\n",
        "\n",
        "\n",
        "* http://www.manythings.org/anki/\n",
        "* https://www.tensorflow.org/text/tutorials/nmt_with_attention"
      ],
      "metadata": {
        "id": "foIt3HDJb6qs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import necessary packages"
      ],
      "metadata": {
        "id": "3pWezfsbdwBB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziRt2kyycr4b",
        "outputId": "ad692c90-a488-4015-837a-3e6a02eb685c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (0.13.0)\n",
            "Requirement already satisfied: tensorflow<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (3.2.2)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.12.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install \"tensorflow-text\"\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ig_mQEsCgdkO"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "from typing import Tuple\n",
        "from typing import List\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import tensorflow_text as tf_text\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import typing\n",
        "import einops\n",
        "import re\n",
        "import pickle\n",
        "import os\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check colab GPU status"
      ],
      "metadata": {
        "id": "v_r_775Td5CG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    # Get the number of available GPUs\n",
        "    num_gpus = len(tf.config.list_physical_devices('GPU'))\n",
        "    print(f\"Number of available GPUs: {num_gpus}\")\n",
        "\n",
        "    # Get the name of the current GPU device\n",
        "    current_gpu_name = tf.config.list_physical_devices('GPU')[0].name\n",
        "    print(f\"Current GPU device: {current_gpu_name}\")\n",
        "else:\n",
        "    print(\"No GPU available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XkBtXRJb4bE",
        "outputId": "830da4ff-ef0f-4d48-bc96-11034c25ff94"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of available GPUs: 1\n",
            "Current GPU device: /physical_device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ShapeChecker class\n",
        "\n",
        "It helps ensure the compatibility of tensor dimensions with named axes, allowing you to catch shape mismatches or inconsistencies during tensor operations."
      ],
      "metadata": {
        "id": "XW48dRxCfQdE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "faG63_qjZCUR"
      },
      "outputs": [],
      "source": [
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    parsed = einops.parse_shape(tensor, names)\n",
        "\n",
        "    for name, new_dim in parsed.items():\n",
        "      old_dim = self.shapes.get(name, None)\n",
        "      \n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        # If the axis name is new, add its length to the cache.\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data processing functions"
      ],
      "metadata": {
        "id": "x20O4rxaf9kl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZJGzNL0dhmZk"
      },
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "  path = Path(path)  # Create a Path object from the string path\n",
        "  text = path.read_text(encoding='utf-8')\n",
        "\n",
        "  lines = text.splitlines()\n",
        "  triplets = [line.split('\\t') for line in lines] # context / target / source (source is not important)\n",
        "\n",
        "  context = []\n",
        "  target = []\n",
        "\n",
        "  for triplet in triplets:\n",
        "    target.append(triplet[0].strip())  # Extract the target and remove leading/trailing whitespaces\n",
        "    context.append(triplet[1].strip())  # Extract the context and remove leading/trailing whitespaces\n",
        "\n",
        "  context = np.array(context)\n",
        "  target = np.array(target)\n",
        "\n",
        "  return target, context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VE-JuUxuoT_E"
      },
      "outputs": [],
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  # Replace Polish letters with Latin letters.\n",
        "  text = tf.strings.regex_replace(text, '[łŁ]', 'l')\n",
        "  text = tf.strings.regex_replace(text, '[ąĄ]', 'a')\n",
        "  text = tf.strings.regex_replace(text, '[ćĆ]', 'c')\n",
        "  text = tf.strings.regex_replace(text, '[ęĘ]', 'e')\n",
        "  text = tf.strings.regex_replace(text, '[ńŃ]', 'n')\n",
        "  text = tf.strings.regex_replace(text, '[óÓ]', 'o') # perhaps it would be wiser to change óÓ to u\n",
        "  text = tf.strings.regex_replace(text, '[śŚ]', 's')\n",
        "  text = tf.strings.regex_replace(text, '[źŹ]', 'z')\n",
        "  text = tf.strings.regex_replace(text, '[żŻ]', 'z')\n",
        "  \n",
        "  # Split accented characters.\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Qtu8IsqwGNWb"
      },
      "outputs": [],
      "source": [
        "def process_text(context, target):\n",
        "  context = context_text_processor(context).to_tensor()\n",
        "  target = target_text_processor(target)\n",
        "  targ_in = target[:,:-1].to_tensor()\n",
        "  targ_out = target[:,1:].to_tensor()\n",
        "  return (context, targ_in), targ_out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural machine translation model architecture"
      ],
      "metadata": {
        "id": "PJ4PV2LQgKMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "CgsErq8HgQML"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zPg1fpowSMs_"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, text_processor, units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.text_processor = text_processor\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    self.units = units\n",
        "    \n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, units,\n",
        "                                               mask_zero=True)\n",
        "\n",
        "    # The RNN layer processes those vectors sequentially.\n",
        "    self.rnn = tf.keras.layers.Bidirectional(\n",
        "        merge_mode='sum',\n",
        "        layer=tf.keras.layers.GRU(units,\n",
        "                            # Return the sequence and state\n",
        "                            return_sequences=True,\n",
        "                            recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "  def call(self, x):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(x, 'batch s')\n",
        "\n",
        "    # The embedding layer looks up the embedding vector for each token.\n",
        "    x = self.embedding(x)\n",
        "    shape_checker(x, 'batch s units')\n",
        "\n",
        "    # The GRU processes the sequence of embeddings.\n",
        "    x = self.rnn(x)\n",
        "    shape_checker(x, 'batch s units')\n",
        "\n",
        "    # Returns the new sequence of embeddings.\n",
        "    return x\n",
        "\n",
        "  def convert_input(self, texts):\n",
        "    texts = tf.convert_to_tensor(texts)\n",
        "    if len(texts.shape) == 0:\n",
        "      texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
        "    context = self.text_processor(texts).to_tensor()\n",
        "    context = self(context)\n",
        "    return context"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross attention"
      ],
      "metadata": {
        "id": "mBIjsS3PgUuC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "THJeJmaZI-QW"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, x, context):\n",
        "    shape_checker = ShapeChecker()\n",
        " \n",
        "    shape_checker(x, 'batch t units')\n",
        "    shape_checker(context, 'batch s units')\n",
        "\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "    \n",
        "    shape_checker(x, 'batch t units')\n",
        "    shape_checker(attn_scores, 'batch heads t s')\n",
        "    \n",
        "    # Cache the attention scores for plotting later.\n",
        "    attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
        "    shape_checker(attn_scores, 'batch t s')\n",
        "    self.last_attention_weights = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "L2XLqrtZgYt3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NfYg7IO6OM25"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, text_processor, units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.text_processor = text_processor\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    self.word_to_id = tf.keras.layers.StringLookup(\n",
        "        vocabulary=text_processor.get_vocabulary(),\n",
        "        mask_token='', oov_token='[UNK]')\n",
        "    self.id_to_word = tf.keras.layers.StringLookup(\n",
        "        vocabulary=text_processor.get_vocabulary(),\n",
        "        mask_token='', oov_token='[UNK]',\n",
        "        invert=True)\n",
        "    self.start_token = self.word_to_id('[START]')\n",
        "    self.end_token = self.word_to_id('[END]')\n",
        "\n",
        "    self.units = units\n",
        "\n",
        "\n",
        "    # 1. The embedding layer converts token IDs to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n",
        "                                               units, mask_zero=True)\n",
        "\n",
        "    # The RNN keeps track of what's been generated so far.\n",
        "    self.rnn = tf.keras.layers.GRU(units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    # The RNN output will be the query for the attention layer.\n",
        "    self.attention = CrossAttention(units)\n",
        "\n",
        "    # This fully connected layer produces the logits for each\n",
        "    # output token.\n",
        "    self.output_layer = tf.keras.layers.Dense(self.vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "502k6TAiaMqw"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def call(self,\n",
        "         context, x,\n",
        "         state=None,\n",
        "         return_state=False):  \n",
        "  shape_checker = ShapeChecker()\n",
        "  shape_checker(x, 'batch t')\n",
        "  shape_checker(context, 'batch s units')\n",
        "\n",
        "  # Lookup the embeddings\n",
        "  x = self.embedding(x)\n",
        "  shape_checker(x, 'batch t units')\n",
        "\n",
        "  # Process the target sequence.\n",
        "  x, state = self.rnn(x, initial_state=state)\n",
        "  shape_checker(x, 'batch t units')\n",
        "\n",
        "  # Use the RNN output as the query for the attention over the context.\n",
        "  x = self.attention(x, context)\n",
        "  self.last_attention_weights = self.attention.last_attention_weights\n",
        "  shape_checker(x, 'batch t units')\n",
        "  shape_checker(self.last_attention_weights, 'batch t s')\n",
        "\n",
        "  # Generate logit predictions for the next token.\n",
        "  logits = self.output_layer(x)\n",
        "  shape_checker(logits, 'batch t target_vocab_size')\n",
        "\n",
        "  if return_state:\n",
        "    return logits, state\n",
        "  else:\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ScvlvCRkg5Ke"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def get_initial_state(self, context):\n",
        "  batch_size = tf.shape(context)[0]\n",
        "  start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "  embedded = self.embedding(start_tokens)\n",
        "  return start_tokens, done, self.rnn.get_initial_state(embedded)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Bzl4tOhQhtoq"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def tokens_to_text(self, tokens):\n",
        "  words = self.id_to_word(tokens)\n",
        "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "  result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
        "  result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UX0xAPECipef"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
        "  logits, state = self(\n",
        "    context, next_token,\n",
        "    state = state,\n",
        "    return_state=True) \n",
        "  \n",
        "  if temperature == 0.0:\n",
        "    next_token = tf.argmax(logits, axis=-1)\n",
        "  else:\n",
        "    logits = logits[:, -1, :]/temperature\n",
        "    next_token = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "  # If a sequence produces an `end_token`, set it `done`\n",
        "  done = done | (next_token == self.end_token)\n",
        "  # Once a sequence is done it only produces 0-padding.\n",
        "  next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
        "  \n",
        "  return next_token, done, state"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translator model"
      ],
      "metadata": {
        "id": "td6RxTcOgcTl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "I8P5ryrMleNR"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, units,\n",
        "               context_text_processor,\n",
        "               target_text_processor):\n",
        "    super().__init__()\n",
        "    # Build the encoder and decoder\n",
        "    encoder = Encoder(context_text_processor, units)\n",
        "    decoder = Decoder(target_text_processor, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def call(self, inputs):\n",
        "    context, x = inputs\n",
        "    context = self.encoder(context)\n",
        "    logits = self.decoder(context, x)\n",
        "\n",
        "    #TODO(b/250038731): remove this\n",
        "    try:\n",
        "      # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GlZF0QPdszy3"
      },
      "outputs": [],
      "source": [
        "@Translator.add_method\n",
        "def translate(self,\n",
        "              texts,\n",
        "              *,\n",
        "              max_length=500,\n",
        "              temperature=tf.constant(0.0)):\n",
        "  shape_checker = ShapeChecker()\n",
        "  context = self.encoder.convert_input(texts)\n",
        "  batch_size = tf.shape(context)[0]\n",
        "  shape_checker(context, 'batch s units')\n",
        "\n",
        "  next_token, done, state = self.decoder.get_initial_state(context)\n",
        "\n",
        "  # initialize the accumulator\n",
        "  tokens = tf.TensorArray(tf.int64, size=1, dynamic_size=True)\n",
        "\n",
        "  for t in tf.range(max_length):\n",
        "    # Generate the next token\n",
        "    next_token, done, state = self.decoder.get_next_token(\n",
        "        context, next_token, done, state, temperature)\n",
        "    shape_checker(next_token, 'batch t1')\n",
        "\n",
        "    # Collect the generated tokens\n",
        "    tokens = tokens.write(t, next_token)\n",
        "\n",
        "    # if all the sequences are done, break\n",
        "    if tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  # Convert the list of generated token ids to a list of strings.\n",
        "  tokens = tokens.stack()\n",
        "  shape_checker(tokens, 't batch t1')\n",
        "  tokens = einops.rearrange(tokens, 't batch 1 -> batch t')\n",
        "  shape_checker(tokens, 'batch t')\n",
        "\n",
        "  text = self.decoder.tokens_to_text(tokens)\n",
        "  shape_checker(text, 'batch')\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model training metrics"
      ],
      "metadata": {
        "id": "px1dmUu6ggpb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SVFPBOjWm4et"
      },
      "outputs": [],
      "source": [
        "def masked_loss(y_true, y_pred):\n",
        "  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                          reduction=\"none\")\n",
        "  loss = loss_fn(y_true, y_pred)\n",
        "\n",
        "  mask = tf.cast(y_true != 0, loss.dtype)\n",
        "  loss = tf.multiply(loss, mask)\n",
        "\n",
        "  return tf.reduce_sum(loss) / tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AAxuT1Vrnubl"
      },
      "outputs": [],
      "source": [
        "def masked_acc(y_true, y_pred):\n",
        "  y_pred = tf.argmax(y_pred, axis=-1)\n",
        "  y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "\n",
        "  match_ = tf.cast(y_true == y_pred, tf.float32)\n",
        "  mask = tf.cast(y_true != 0, tf.float32)\n",
        "\n",
        "  return tf.reduce_sum(match_)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translation class\n",
        "Probably could have just been a child class / extension of Translator class"
      ],
      "metadata": {
        "id": "dMAmQAyKgplh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Translation():\n",
        "  def __init__(self, model: Translator=None, model_file_path: str=None, model_folder_path: str=None,\n",
        "               context: List[str]=None, target: List[str]=None):\n",
        "    print(\"Initializing translation model...\")\n",
        "    self.model = model\n",
        "    self.model_folder_path = model_folder_path\n",
        "    self.context = context \n",
        "    self.target = target\n",
        "\n",
        "  def translate_sentence(self, text: str) -> str:\n",
        "    # Check if input is of type string\n",
        "    if not isinstance(text, str):\n",
        "      raise ValueError(\"Input must be a string.\")\n",
        "    # Check if multiple sentences are present\n",
        "    if re.search(r'\\.\\s|!\\s|\\?\\s', text):\n",
        "      raise ValueError(\"Multiple sentences detected. \\\n",
        "                          Please provide a single sentence.\")\n",
        "\n",
        "    result = self.model.translate([text])\n",
        "    result = result[0].numpy().decode()\n",
        "    result = result.capitalize()\n",
        "    result = re.sub(r'\\s+([.!?])', r'\\1', result)\n",
        "\n",
        "    return result\n",
        "\n",
        "  def split_sentences(self, text: str) -> list:\n",
        "    # Define the regex pattern to match sentence boundaries\n",
        "    pattern = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s'\n",
        "\n",
        "    # Split the text into sentences using the regex pattern\n",
        "    sentences = re.split(pattern, text)\n",
        "\n",
        "    return sentences\n",
        "\n",
        "  def translate_text(self, text: str) -> str:\n",
        "    sentences = self.split_sentences(text)\n",
        "\n",
        "    result = \"\"\n",
        "    \n",
        "    for sentence in sentences:\n",
        "      result +=  self.translate_sentence(sentence)\n",
        "    \n",
        "    return result\n",
        "\n",
        "  def teach_model(self):\n",
        "    while True:\n",
        "      input_text = input(\"Enter input text: \")\n",
        "      translated_text = self.translate_text(input_text)\n",
        "      print(f\"Translated text: {translated_text}\")\n",
        "\n",
        "      correct = input(\"Is the translation correct? (yes/no): \")\n",
        "      if correct.lower() == \"yes\":\n",
        "        self.context = np.append(self.context, input_text)\n",
        "        self.target = np.append(self.target, translated_text)\n",
        "        print(\"Context and target added.\")\n",
        "      elif correct.lower() == \"no\":\n",
        "        correct_translation = input(\"Enter the correct translation: \")\n",
        "        self.context = np.append(self.context, input_text)\n",
        "        self.target = np.append(self.target, correct_translation)\n",
        "        print(\"Correct translation added.\")\n",
        "      else:\n",
        "        print(\"Invalid input. Please enter 'yes' or 'no'.\")\n",
        "\n",
        "      continue_teaching = input(\"Do you want to teach the model more? (yes/no): \")\n",
        "      if continue_teaching.lower() != \"yes\":\n",
        "        break\n",
        "\n",
        "  def load_model(self):\n",
        "    file_path = os.path.join(self.model_file_path, \"model.pkl\")\n",
        "    if os.path.exists(file_path):\n",
        "      with open(file_path, 'rb') as file:\n",
        "        self.model = pickle.load(file)\n",
        "      print(\"Model loaded successfully.\")\n",
        "      return model\n",
        "    else:\n",
        "      raise FileNotFoundError(f\"Model file '{file_path}' not found.\")\n",
        "\n",
        "  def save_model(self):\n",
        "    current_datetime = datetime.datetime.now()\n",
        "\n",
        "    filename = f\"model_{current_datetime.year}_{current_datetime.month}_\" \\\n",
        "               f\"{current_datetime.day}_{current_datetime.hour}_\" \\\n",
        "               f\"{current_datetime.minute}_{current_datetime.second}.pkl\"\n",
        "    file_path = os.path.join(self.model_folder_path, filename)\n",
        "\n",
        "    with open(file_path, 'wb') as file:\n",
        "      pickle.dump(self.model, file)\n",
        "    print(f\"Model saved successfully at {file_path}.\")\n",
        "\n",
        "  def train_model(self):\n",
        "    is_train = np.random.uniform(size=(len(self.target),)) < 0.8\n",
        "\n",
        "    train_raw = (tf.data.Dataset\n",
        "                .from_tensor_slices((self.context[is_train], self.target[is_train]))\n",
        "                .shuffle(BUFFER_SIZE)\n",
        "                .batch(BATCH_SIZE))\n",
        "    \n",
        "    val_raw = (tf.data.Dataset\n",
        "              .from_tensor_slices((self.context[~is_train], self.target[~is_train]))\n",
        "              .shuffle(BUFFER_SIZE)\n",
        "              .batch(BATCH_SIZE))\n",
        "\n",
        "    train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
        "    val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)\n",
        "\n",
        "    self.model = Translator(UNITS,\n",
        "                      context_text_processor,\n",
        "                      target_text_processor)\n",
        "    \n",
        "    self.model.compile(optimizer=\"adam\",\n",
        "                  loss=masked_loss,\n",
        "                  metrics=[masked_acc, masked_loss])\n",
        "\n",
        "    self.model.fit(train_ds.repeat(),\n",
        "          epochs=100,\n",
        "          steps_per_epoch=100,\n",
        "          validation_data=val_ds,\n",
        "          validation_steps=20,\n",
        "          callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)])\n",
        "    \n",
        "    return self.model\n",
        "\n",
        "  def save_data(self, file_path: str):\n",
        "    data = \"\\n\".join([f\"{context} {target}\" for context, target in zip(self.context, self.target)])\n",
        "\n",
        "    with open(file_path, 'w') as file:\n",
        "      file.write(data)\n",
        "\n",
        "    print(f\"Data saved successfully to {file_path}.\")"
      ],
      "metadata": {
        "id": "fVrdbNeBBdQ7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main build"
      ],
      "metadata": {
        "id": "f_DmvpITg4DH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data processing"
      ],
      "metadata": {
        "id": "WbeL2LXVg7H0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "t9uZfOKVj16l"
      },
      "outputs": [],
      "source": [
        "target_raw, context_raw = load_data(\"/content/pol.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7Ah_m7jxk67p"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(context_raw)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "is_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
        "\n",
        "train_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((context_raw[is_train], target_raw[is_train]))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE))\n",
        "val_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((context_raw[~is_train], target_raw[~is_train]))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZhnTGbISClj7"
      },
      "outputs": [],
      "source": [
        "max_vocab_size = 5000\n",
        "\n",
        "context_text_processor = tf.keras.layers.TextVectorization(standardize=tf_lower_and_split_punct,\n",
        "                                                           max_tokens=max_vocab_size,\n",
        "                                                           ragged=True)\n",
        "\n",
        "target_text_processor = tf.keras.layers.TextVectorization(standardize=tf_lower_and_split_punct,\n",
        "                                                          max_tokens=max_vocab_size,\n",
        "                                                          ragged=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TIt45u8UGKeK"
      },
      "outputs": [],
      "source": [
        "context_text_processor.adapt(train_raw.map(lambda context, target: context))\n",
        "target_text_processor.adapt(train_raw.map(lambda context, target: target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "U7QdZ4FcHn0Y"
      },
      "outputs": [],
      "source": [
        "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
        "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model setup"
      ],
      "metadata": {
        "id": "rOPWhELQhCfp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ncXu2_S1R9tf"
      },
      "outputs": [],
      "source": [
        "UNITS = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "sq6OfOUlo8Fs"
      },
      "outputs": [],
      "source": [
        "model = Translator(UNITS,\n",
        "                   context_text_processor,\n",
        "                   target_text_processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TwzvKwWaoznI"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"adam\",\n",
        "              loss=masked_loss,\n",
        "              metrics=[masked_acc, masked_loss])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "WOezE12fhFkM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0wdXgZWpKfP",
        "outputId": "efcd884d-f0e6-47d4-c03c-f8930d6fa292"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "100/100 [==============================] - 62s 324ms/step - loss: 5.0171 - masked_acc: 0.2609 - masked_loss: 5.0171 - val_loss: 4.0640 - val_masked_acc: 0.3552 - val_masked_loss: 4.0640\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - 8s 81ms/step - loss: 3.7468 - masked_acc: 0.3941 - masked_loss: 3.7468 - val_loss: 3.3791 - val_masked_acc: 0.4295 - val_masked_loss: 3.3791\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 3.2232 - masked_acc: 0.4598 - masked_loss: 3.2232 - val_loss: 2.9716 - val_masked_acc: 0.4901 - val_masked_loss: 2.9716\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - 5s 49ms/step - loss: 2.8821 - masked_acc: 0.5052 - masked_loss: 2.8821 - val_loss: 2.6337 - val_masked_acc: 0.5339 - val_masked_loss: 2.6337\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 2.5984 - masked_acc: 0.5448 - masked_loss: 2.5984 - val_loss: 2.4254 - val_masked_acc: 0.5566 - val_masked_loss: 2.4254\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - 5s 53ms/step - loss: 2.3353 - masked_acc: 0.5812 - masked_loss: 2.3348 - val_loss: 2.1785 - val_masked_acc: 0.5989 - val_masked_loss: 2.1785\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.9291 - masked_acc: 0.6340 - masked_loss: 1.9291 - val_loss: 2.1747 - val_masked_acc: 0.6069 - val_masked_loss: 2.1747\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - 5s 48ms/step - loss: 1.9189 - masked_acc: 0.6345 - masked_loss: 1.9189 - val_loss: 2.0476 - val_masked_acc: 0.6242 - val_masked_loss: 2.0476\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - 5s 51ms/step - loss: 1.8241 - masked_acc: 0.6510 - masked_loss: 1.8241 - val_loss: 1.9607 - val_masked_acc: 0.6342 - val_masked_loss: 1.9607\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - 4s 41ms/step - loss: 1.7490 - masked_acc: 0.6604 - masked_loss: 1.7490 - val_loss: 1.8855 - val_masked_acc: 0.6469 - val_masked_loss: 1.8855\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.6976 - masked_acc: 0.6714 - masked_loss: 1.6976 - val_loss: 1.8010 - val_masked_acc: 0.6598 - val_masked_loss: 1.8010\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - 8s 76ms/step - loss: 1.5654 - masked_acc: 0.6872 - masked_loss: 1.5657 - val_loss: 1.7874 - val_masked_acc: 0.6596 - val_masked_loss: 1.7874\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - 5s 52ms/step - loss: 1.2450 - masked_acc: 0.7302 - masked_loss: 1.2450 - val_loss: 1.7540 - val_masked_acc: 0.6689 - val_masked_loss: 1.7540\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 1.2891 - masked_acc: 0.7206 - masked_loss: 1.2891 - val_loss: 1.6974 - val_masked_acc: 0.6711 - val_masked_loss: 1.6974\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - 4s 39ms/step - loss: 1.3145 - masked_acc: 0.7169 - masked_loss: 1.3145 - val_loss: 1.6744 - val_masked_acc: 0.6768 - val_masked_loss: 1.6744\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - 6s 59ms/step - loss: 1.3178 - masked_acc: 0.7174 - masked_loss: 1.3178 - val_loss: 1.7107 - val_masked_acc: 0.6770 - val_masked_loss: 1.7107\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - 5s 50ms/step - loss: 1.2986 - masked_acc: 0.7222 - masked_loss: 1.2986 - val_loss: 1.6756 - val_masked_acc: 0.6778 - val_masked_loss: 1.6756\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 1.2023 - masked_acc: 0.7389 - masked_loss: 1.2026 - val_loss: 1.6864 - val_masked_acc: 0.6796 - val_masked_loss: 1.6864\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f99212884c0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "model.fit(train_ds.repeat(),\n",
        "          epochs=100,\n",
        "          steps_per_epoch=100,\n",
        "          validation_data=val_ds,\n",
        "          validation_steps=20,\n",
        "          callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "prvCEBuHhIbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translation = Translation(model, context_raw, target_raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuSFyH81zoja",
        "outputId": "0ed1f92c-98fc-4af8-9ab6-2faf9c5a12f0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing translation model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translated_text = translation.translate_text(\"Kocham spacerować boso po trawie. Lubię patrzeć na piękne kwiaty. \\\n",
        "                            Słucham muzyki klasycznej. Cieszę się, że mam przyjaciół. \\\n",
        "                            Uwielbiam jeść frytki.\")\n",
        "print(translated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXtyZMNX3KcQ",
        "outputId": "62be8a25-694d-4e36-858e-d0f6c4e06790"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love walking barefoot on the grass. I like to look at the flowers. Im listening to music. Im glad i have friends. I love to eat strawberries. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "frytki != strawberries"
      ],
      "metadata": {
        "id": "ieQN3WayjNwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translation.teach_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0jFmkglQ9m-",
        "outputId": "03ffb455-7fa8-449f-e234-7c13ba5e04c3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter input text: Idę do kina.\n",
            "Translated text: Im going to the movies. \n",
            "Is the translation correct? (yes/no): yes\n",
            "Context and target added.\n",
            "Do you want to teach the model more? (yes/no): yes\n",
            "Enter input text: Lubię podróżować koleją.\n",
            "Translated text: I like to travel. \n",
            "Is the translation correct? (yes/no): no\n",
            "Enter the correct translation: I like to travel by rail.\n",
            "Correct translation added.\n",
            "Do you want to teach the model more? (yes/no): yes\n",
            "Enter input text: Czy jesteś dobrą osobą?\n",
            "Translated text: Are you a good person? \n",
            "Is the translation correct? (yes/no): yes\n",
            "Context and target added.\n",
            "Do you want to teach the model more? (yes/no): yes\n",
            "Enter input text: Czy wyjdziesz za mnie?\n",
            "Translated text: Are you angry with me? \n",
            "Is the translation correct? (yes/no): no\n",
            "Enter the correct translation: Will you mary me?\n",
            "Correct translation added.\n",
            "Do you want to teach the model more? (yes/no): yes\n",
            "Enter input text: Jestem bardzo niezadowolony dzisiaj.\n",
            "Translated text: Im very dissatisfied today. \n",
            "Is the translation correct? (yes/no): yes\n",
            "Context and target added.\n",
            "Do you want to teach the model more? (yes/no): Chcę pojechać nad morze.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}